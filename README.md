# Machine Comprehension
Understanding Of Machine Comprehension and Question Answering System

Summarization of SQuAD: 100,000+ Questions for Machine Comprehension of Text

SQuAD Dataset

## What is SQuAD

SQuAD is Stanford Question Answering Dataset for Reading Comprehension.
Reading Comprehension is the ability to understand a given passage and answer questions based on that passage.
This is a complex and challenging task for machines as it requires understanding of language and about the world.

This Dataset is developed to make progress in the space of Machine Comprehension.
It consists of more than 100k questions based on 536 Wikipedia Articles. 
Answers to each question is a segment of text or span from corresponding reading passage. 

## Why SQuAD ?

Existing Reading Comprehension datasets have certain limitations 

* High quality datasets are small in number and for deep learning based approach one requirement is large amount of data. 
* Datasets which are large are semi-synthetic and does not share characteristics of explicit reading comprehensions questions.

SQuAD does not provide list of answer choices each question like previous datasets but the system selects all possible answers from all possible spans in the passage. One limitaion with span based answers is it is more constrained.
One of the benefit of span based answers is it is easier to evaluate than free- form answers.

## Dataset Collection Approach

It is a 3 step process.
* Curating Passages
* Crowd Sourcing Questions , Answers on those passages.
* Obtaining additional answers.

### Passage Curation
To retrieve high quality articles Wikipedia's internal PageRanks was used to retrieve 10k articles from which 536 articles were sampled randomly. In the dataprocessing step individual paragraphs were extracted and other contents like images , tables etc were removed. Paragraphs less than 500 characters were discarded. Overall there were 23k paragraphs.

### Question-answer Collection

Crowdworkers were asked to create 5 questions and answers based on a paragraph. Questions were entered in text field and answers were highlighted in paragraph. Crowdworkers were encouraged to ask questions in their own words rather than reusing or copying the phrases from the paragraph.

### Additional answers collection

In this step crowdworker was shown only the questions corresponding to a paragraph and select the shortest span in paragraph which answered the question.If question was not answered by a span in paragraph it was submitted without an answer.

## Dataset Analysis

To analyse the the questions and answers three different properties were explored.

* Diversity in Answer : Answers were categorized into numerical and non-numerical automatically. Non-numerical answers were further categorized using POS tags generated by Stanford CoreNLP. eg :  Prper noun phrase was further splitted into person, location and other entities using NER tags.

* Reasoning required to answer question : To get better understanding of reasoning required to answer the question 4 questions were sampled from 48 articles in dev set and then manually labeled with categories. e.g : Lexical Variation(Synonym), Lexical Variation(world knowledge), Syntactic variation, Multiple sentence reasoning, ambiguous.
Results showed that examples had some sort oflexical or syntactic divergence.

* Syntactic divergence between the question and answer sentences : An automatic method was developed to quantify the syntactic divergence between question and sentence containing answers.
First detect anchors(word-lemma pairs common to both the question and answer), unlexicalized paths are extracted from anchors then edit distance is measured beteen the paths. Syntactic divergence is defined as minimum edit distance over all possible anchors. There is wide range of syntactic divergence present in dataset.

## Methods

Logistic Regression model was developed and its performance was compared to 3 other baseline models.
In Logistic Regression model several types of features were extracted for each candidate answer. e.g Matching word frequencies, Matching bigram frequencies , Root Match , Lengths etc


## Model Evaluation

Two different metrics were used to evaluate model accuracy.

* Exact Match : Percentage of predictions that matches the exact ground truth.
* F1 Score : It measures the average overlap between the the prediction and ground truth answer. Prediction and ground truth were treated as bag of tokens and compute their F1 score.

# Result Analysis

* Feature Ablation : In order to understand the features that are responsible for the performance of logistic regression model feature ablation was performed where one group of feature was removed from the model at a given time. Results showed that lexicalized and dependency tree path features are most important.

* Performance stratified by answer type : The logistic regression model performs best on dates and number categories for which answers are single tokens. Model is challenged more on named entities where there are more plausible candidates.

* Performance startified by syntactic Divergence : More the syntactic divergence between the questions and answers lower the performance of model.














